{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "Group n-grams if in dependency tree they are either\n",
    "    1. Direct children with only one link between them or\n",
    "    2. Leafs on the same level with the same parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "from typing import List, Tuple\n",
    "nlp = spacy.load('en')\n",
    "stopword = \"the in has be\".split()\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('bit', 2)],\n",
       " [('me', 3)],\n",
       " [('dog', 1), ('The', 0)],\n",
       " [('hard', 5), ('very', 4)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_n_gram_generate(node, pos_tagged_n_grams=None):\n",
    "    \n",
    "    if pos_tagged_n_grams is None:\n",
    "        pos_tagged_n_grams = []\n",
    "\n",
    "    current_node = node\n",
    "    backlog = []\n",
    "    while current_node.n_lefts + current_node.n_rights == 1:\n",
    "        backlog.append((current_node.orth_, current_node.i))\n",
    "        current_node = list(current_node.children)[0]\n",
    "\n",
    "    backlog.append((current_node.orth_, current_node.i))\n",
    "    if current_node.n_lefts + current_node.n_rights > 1:\n",
    "        good_children = [child for child in current_node.children if len(list(child.children)) > 0]\n",
    "        bad_children = [(child.orth_, child.i) for child in current_node.children if child not in good_children]\n",
    "        pos_tagged_n_grams.append(backlog)\n",
    "        pos_tagged_n_grams.append(bad_children)\n",
    "        \n",
    "        for child in good_children:\n",
    "            to_nltk_tree(child, pos_tagged_n_grams)\n",
    "    else:\n",
    "        pos_tagged_n_grams.append(backlog)\n",
    "\n",
    "    return pos_tagged_n_grams\n",
    "        \n",
    "keep_stop_words = True\n",
    "sentence = \"The dog bit me very hard\"\n",
    "query = \" \".join([word for word in sentence.split() if word not in stopword or keep_stop_words])\n",
    "doc = nlp(query)\n",
    "print(type(list(doc.sents)[0].root))\n",
    "to_nltk_tree(list(doc.sents)[0].root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matt has already', 'completed', 'the', 'homework', 'for the class']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_n_gram(sentence: str, keep_stop_words: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate an n-gram based on the POS tagged dependency tree of the sentence that is \"simplified\" down according\n",
    "    to a few assumptions that dictate a good sentence split. These assumptions are as follows:\n",
    "        1. If two words are leafs and on the same level with the same parent they can be grouped as an n-gram\n",
    "        2. If there is a sequence of parent-child relationships with only 1 child they can be grouped as one\n",
    "           n-gram\n",
    "           \n",
    "    \n",
    "    \"\"\"\n",
    "    pos_tagged_n_grams = []\n",
    "    \n",
    "    def to_nltk_tree(node):\n",
    "        current_node = node\n",
    "        backlog = []\n",
    "        while current_node.n_lefts + current_node.n_rights == 1:\n",
    "            backlog.append((current_node.orth_, current_node.i))\n",
    "            current_node = list(current_node.children)[0]\n",
    "\n",
    "        backlog.append((current_node.orth_, current_node.i))\n",
    "        if current_node.n_lefts + current_node.n_rights > 1:\n",
    "            good_children = [child for child in current_node.children if len(list(child.children)) > 0]\n",
    "            bad_children = [(child.orth_, child.i) for child in current_node.children if child not in good_children]\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            pos_tagged_n_grams.append(bad_children)\n",
    "            return Tree(backlog, [Tree(bad_children, [])] + [to_nltk_tree(child) for child in good_children])\n",
    "        else:\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            return Tree(backlog, [])\n",
    "        \n",
    "    def strip_nothing_unigrams(n_grams):\n",
    "        return [n_gram for n_gram in n_grams if not (len(n_gram.split(\" \")) == 1 and n_gram.split(\" \")[0] in stopword)]\n",
    "\n",
    "    query = \" \".join([word for word in sentence.split() if word not in stopword or keep_stop_words])\n",
    "    doc = nlp(query)\n",
    "    to_nltk_tree(list(doc.sents)[0].root)\n",
    "    # print(nltk_tree)\n",
    "\n",
    "    sort_inner = [sorted(nltk_child, key=lambda x: x[1]) for nltk_child in pos_tagged_n_grams]\n",
    "\n",
    "    nltk_averages = []\n",
    "    for nltk_child in sort_inner:\n",
    "        nltk_averages.append((nltk_child, max(x[1] for x in nltk_child)))\n",
    "\n",
    "    sorted_outer = list(sorted(nltk_averages, key=lambda x: x[1]))\n",
    "\n",
    "    n_grams = []\n",
    "    for nltk_average in sorted_outer:\n",
    "        n_grams.append(\" \".join(word[0] for word in nltk_average[0]))\n",
    "        \n",
    "    return n_grams\n",
    "        \n",
    "pos_n_gram(\"Matt has already completed the homework for the class\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ddfd23d7d43f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu\"Matt has already completed the homework for the class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pp' is not defined"
     ]
    }
   ],
   "source": [
    "query = u\"Matt has already completed the homework for the class\"\n",
    "pp(query)\n",
    "query = \" \".join([word for word in query.split() if word not in stopword])\n",
    "pp(query)\n",
    "doc = nlp(query)\n",
    "nltk_tree = to_nltk_tree(list(doc.sents)[0].root)\n",
    "print(nltk_tree.leaves() + [nltk_tree.label()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_ + \" \" + node.pos_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_ + \" \" + node.pos_\n",
    "\n",
    "\n",
    "def pp(query):\n",
    "    doc = nlp(query)\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('finished', 1)], 1)\n",
      "([('Matt', 0), ('homework', 2)], 2)\n",
      "([('before', 3), ('class', 4), ('even', 5)], 5)\n",
      "([('started', 6)], 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['finished', 'Matt homework', 'before class even', 'started']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = u'Matt finished the homework before class even started'\n",
    "# pp(query)\n",
    "query = \" \".join([word for word in query.split() if word not in stopword])\n",
    "doc = nlp(query)\n",
    "nltk_tree = to_tree(list(doc.sents)[0].root);\n",
    "nltk_averages = []\n",
    "# print(nltk_tree)\n",
    "\n",
    "sort_inner = [sorted(nltk_child, key=lambda x: x[1]) for nltk_child in nltk_tree]\n",
    "\n",
    "for nltk_child in sort_inner:\n",
    "    nltk_averages.append((nltk_child, max(x[1] for x in nltk_child)))\n",
    "\n",
    "\n",
    "sorted_outer = list(sorted(nltk_averages, key=lambda x: x[1]))\n",
    "[print(x) for x in sorted_outer]\n",
    "\n",
    "\n",
    "\n",
    "n_grams = []\n",
    "for nltk_average in sorted_outer:\n",
    "    n_grams.append(\" \".join(word[0] for word in nltk_average[0]))\n",
    "\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     finished VERB                        \n",
      "   ________________________|____________                   \n",
      "  |    homework NOUN               started VERB           \n",
      "  |          |              ____________|___________       \n",
      "I PRON    the DET       just ADV    before ADP  class NOUN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"I finished the homework just before class started\"\n",
    "pp(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
