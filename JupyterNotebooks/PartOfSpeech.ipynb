{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "Group n-grams if in dependency tree they are either\n",
    "    1. Direct children with only one link between them or\n",
    "    2. Leafs on the same level with the same parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "from typing import List\n",
    "nlp = spacy.load('en')\n",
    "stopword = \"the in has be\".split()\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matt has already', 'completed', 'the', 'homework', 'for the class']"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_n_gram(sentence: str, keep_stop_words: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate an n-gram based on the POS tagged dependency tree of the sentence that is \"simplified\" down according\n",
    "    to a few assumptions that dictate a good sentence split. These assumptions are as follows:\n",
    "        1. If two words are leafs and on the same level with the same parent they can be grouped as an n-gram\n",
    "        2. If there is a sequence of parent-child relationships with only 1 child they can be grouped as one\n",
    "           n-gram\n",
    "           \n",
    "    \n",
    "    \"\"\"\n",
    "    pos_tagged_n_grams = []\n",
    "    \n",
    "    def to_nltk_tree(node):\n",
    "        current_node = node\n",
    "        backlog = []\n",
    "        while current_node.n_lefts + current_node.n_rights == 1:\n",
    "            backlog.append((current_node.orth_, current_node.i))\n",
    "            current_node = list(current_node.children)[0]\n",
    "\n",
    "        backlog.append((current_node.orth_, current_node.i))\n",
    "        if current_node.n_lefts + current_node.n_rights > 1:\n",
    "            good_children = [child for child in current_node.children if len(list(child.children)) > 0]\n",
    "            bad_children = [(child.orth_, child.i) for child in current_node.children if child not in good_children]\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            pos_tagged_n_grams.append(bad_children)\n",
    "            return Tree(backlog, [Tree(bad_children, [])] + [to_nltk_tree(child) for child in good_children])\n",
    "        else:\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            return Tree(backlog, [])\n",
    "        \n",
    "    def strip_nothing_unigrams(n_grams):\n",
    "        return [n_gram for n_gram in n_grams if not (len(n_gram.split(\" \")) == 1 and n_gram.split(\" \")[0] in stopword)]\n",
    "\n",
    "    query = \" \".join([word for word in sentence.split() if word not in stopword or keep_stop_words])\n",
    "    doc = nlp(query)\n",
    "    to_nltk_tree(list(doc.sents)[0].root);\n",
    "    # print(nltk_tree)\n",
    "\n",
    "    sort_inner = [sorted(nltk_child, key=lambda x: x[1]) for nltk_child in pos_tagged_n_grams]\n",
    "\n",
    "    nltk_averages = []\n",
    "    for nltk_child in sort_inner:\n",
    "        nltk_averages.append((nltk_child, max(x[1] for x in nltk_child)))\n",
    "\n",
    "    sorted_outer = list(sorted(nltk_averages, key=lambda x: x[1]))\n",
    "\n",
    "    n_grams = []\n",
    "    for nltk_average in sorted_outer:\n",
    "        n_grams.append(\" \".join(word[0] for word in nltk_average[0]))\n",
    "        \n",
    "    return n_grams\n",
    "        \n",
    "pos_n_gram(\"Matt has already completed the homework for the class\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         completed                   \n",
      "  ___________|____________            \n",
      " |    |      |         homework      \n",
      " |    |      |       _____|_______    \n",
      " |    |      |      |            for \n",
      " |    |      |      |             |   \n",
      " |    |      |      |           class\n",
      " |    |      |      |             |   \n",
      "Matt has  already  the           the \n",
      "\n",
      "     completed         \n",
      "  _______|________      \n",
      " |       |     homework\n",
      " |       |        |     \n",
      " |       |       for   \n",
      " |       |        |     \n",
      "Matt  already   class  \n",
      "\n",
      "completed\n",
      "[\"[('class', 5), ('homework', 3), ('for', 4)]\", \"[('completed', 2)]\"]\n"
     ]
    }
   ],
   "source": [
    "query = u''\n",
    "pp(query)\n",
    "query = \" \".join([word for word in query.split() if word not in stopword])\n",
    "pp(query)\n",
    "doc = nlp(query)\n",
    "nltk_tree = to_nltk_tree(list(doc.sents)[0].root)\n",
    "print(nltk_tree.leaves() + [nltk_tree.label()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(query):\n",
    "    doc = nlp(query)\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('finished', 1)], 1)\n",
      "([('Matt', 0), ('homework', 2)], 2)\n",
      "([('before', 3), ('class', 4), ('even', 5)], 5)\n",
      "([('started', 6)], 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['finished', 'Matt homework', 'before class even', 'started']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = u'Matt finished the homework before class even started'\n",
    "# pp(query)\n",
    "query = \" \".join([word for word in query.split() if word not in stopword])\n",
    "doc = nlp(query)\n",
    "nltk_tree = to_tree(list(doc.sents)[0].root);\n",
    "nltk_averages = []\n",
    "# print(nltk_tree)\n",
    "\n",
    "sort_inner = [sorted(nltk_child, key=lambda x: x[1]) for nltk_child in nltk_tree]\n",
    "\n",
    "for nltk_child in sort_inner:\n",
    "    nltk_averages.append((nltk_child, max(x[1] for x in nltk_child)))\n",
    "\n",
    "\n",
    "sorted_outer = list(sorted(nltk_averages, key=lambda x: x[1]))\n",
    "[print(x) for x in sorted_outer]\n",
    "\n",
    "\n",
    "\n",
    "n_grams = []\n",
    "for nltk_average in sorted_outer:\n",
    "    n_grams.append(\" \".join(word[0] for word in nltk_average[0]))\n",
    "\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.leaves of Tree([('is', 2)], [Tree([], []), Tree([('defense', 1), ('Unilateral', 0)], []), Tree([('idea', 5)], [Tree([('a', 3), ('bad', 4)], [])])])>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tree.leaves()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
