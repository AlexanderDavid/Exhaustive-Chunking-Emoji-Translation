{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Sentence to Emoji Translation \n",
    "## Purpose\n",
    "To workshop a naive version of an sentence to emoji translation algorithm. The general idea is that sentences can be \"chuncked\" out into n-grams that are more related to a single emoji. The related-ness of an n-gram to an emoji is directly related to the cosine similarity of the sent2vec representation of the sentence and the sent2vec representation of one of the emoji's definitions. The emoji definitons are gathered from the [emoji2vec](https://github.com/uclmr/emoji2vec) github repo and the sent2vec model is from the [sent2vec](https://github.com/epfml/sent2vec) github repo. \n",
    "\n",
    "## Issues\n",
    "- The generation of the summary is so incredibly slow\n",
    "- There are some issues with lemmatization (e.g. poop != pooped when lemmatized)\n",
    "- /opt/conda/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
    "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
    "\n",
    "## Ideas\n",
    "- Add bias for fewer emojis. Some of the generated sentences are just the sentence translated into 1-grams and  it is really easy to find an emoji that represents a one word. If some how the sentence was scored both based on sum similarity and the length of the sentence that might produce better summarizations\n",
    "- Use a larger sent2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.17.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy) (4.32.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (0.8.5)\n",
      "Processing /home/jovyan/work/sent2vec\n",
      "Building wheels for collected packages: sent2vec\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp37-cp37m-linux_x86_64.whl size=1174144 sha256=c46cb69ebec049684f0545667874fd23210a5c3f2e6fbd55678d638f4f342577\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-inl44dua/wheels/d9/e7/26/888a49dfb31f02930dae17493fe08a50bfd1fb09ab9959ea60\n",
      "Successfully built sent2vec\n",
      "Installing collected packages: sent2vec\n",
      "  Found existing installation: sent2vec 0.0.0\n",
      "    Uninstalling sent2vec-0.0.0:\n",
      "      Successfully uninstalled sent2vec-0.0.0\n",
      "Successfully installed sent2vec-0.0.0\n"
     ]
    }
   ],
   "source": [
    "# Installs TODO: Add these to docker\n",
    "!pip install spacy\n",
    "!pip install tabulate\n",
    "!pip install ../sent2vec/.\n",
    "\n",
    "# Standard Library\n",
    "from typing import List, Tuple, Callable # Datatypes for the function typing\n",
    "from functools import lru_cache          # Function annotation for storing results \n",
    "from dataclasses import dataclass, field # C-like struct functions and class annotation\n",
    "from string import punctuation\n",
    "\n",
    "# Scipy suite\n",
    "import numpy as np                        # For function annotation\n",
    "from scipy.spatial.distance import cosine # Distance between sentence and emoji in sent2vec vector space\n",
    "\n",
    "# NLTK \n",
    "from nltk import word_tokenize                                          # Tokenizing a sentence into words\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer # Different stemming algorithms\n",
    "from nltk.corpus import stopwords                                       # Define the set of stopwords in english\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Import spacy (NLP)\n",
    "import spacy\n",
    "\n",
    "# Import sentence vectorizer\n",
    "import sent2vec\n",
    "\n",
    "# IPython output formatting\n",
    "from tabulate import tabulate                           # Tabulation from 2-d array into html table\n",
    "from IPython.display import display, HTML, clear_output # Nice displaying in the output cell\n",
    "import warnings; warnings.simplefilter('ignore')        # cosine distance gives warnings when div by 0 so\n",
    "                                                        # ignore all of these\n",
    "    \n",
    "# Timing functions\n",
    "from time import time, localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramatize the file locations\n",
    "emoji_file = \"./data/emoji_joined_emojipedia.txt\" # https://github.com/uclnlp/emoji2vec/blob/master/data/raw_training_data/emoji_joined.txt\n",
    "wikipedia_file = \"./data/wikipedia_utf8_filtered_20pageviews.csv\" # https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sent2vec model\n",
    "s2v = sent2vec.Sent2vecModel()\n",
    "s2v.load_model('../models/wiki_unigrams.bin') # https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0MB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.17.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.32.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.6.16)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-cp37-none-any.whl size=12019126 sha256=ca8e20cf90773315a79c147c1e2b2060e7a9006529c1e84e992a8574d1ef79b5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cpbuy27f/wheels/48/5c/1c/15f9d02afc8221a668d2172446dd8467b20cdb9aef80a172a4\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# Intitialize the lemmatizers\n",
    "!python -m spacy download en\n",
    "lemmatizerSpacy = spacy.load('en', disable=['parser', 'ner'])\n",
    "ps = PorterStemmer()\n",
    "sb = SnowballStemmer(\"english\")\n",
    "lemmatizerNLTK = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Cleaning\n",
    "The general idea with sentence cleaning is that the sentences need to be put into the same \"format\" for better analysis. There are two main aspects of cleaning: 1) removal, and 2) modification. Removal is primarily for tokens that do not contribute to the sentence at all. These include \".\", \"and\", \"but\". Normally this is a standard step in sentence cleaning but it has actually has zero effect on the output that I can see. However, token modification changes the spelling of tokens to uniform all tokens that use the same root. For example \"rocked\", \"rock\", \"rocking\" should all be reduced to their lemma of \"rock\". There are two different ways to do this: [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent: str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Clean a sentence\n",
    "    \n",
    "    Tokenize the word and then lemmatize each individual word before rejoining it all together.\n",
    "    Optionally removing stop words along the way\n",
    "        \n",
    "    Args:\n",
    "        sent(str): Sentence to clean\n",
    "        lemma_func(Callable[[str], str]): A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the sentence\n",
    "    Rets:\n",
    "        (str): Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Lemmatize each word in the sentence and remove the stop words if the flag is set\n",
    "    return \" \".join([lemma_func(token) for token in word_tokenize(sent.lower()) if (token not in stopwords or keep_stop_words) and (token not in punctuation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoji Vectorization and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array to store the (emoji, repr) 2-tuple\n",
    "def generate_emoji_embeddings(lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Generate the sent2vec emoji embeddings from the input file\n",
    "    \n",
    "    Run each emoji within the emoji_joined data file from the emoji2vec paper through\n",
    "    the sent2vec sentence embedder. This is a very naive way of doing it because one\n",
    "    emoji may have multiple entries in the data file so it has multiple vectors in the\n",
    "    emoji_embeddings array\n",
    "    \n",
    "    Args:\n",
    "        lemma_func(Callable[[str], str]): Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the cleaned sentence\n",
    "    Rets:\n",
    "        (List[Tuple[str, List[float]]]): A list of 2-tuples containing the emoji and \n",
    "                                         one vector representation of it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list that will hold all of the embedings\n",
    "    emoji_embeddings = []\n",
    "    \n",
    "    # Open the file that stores the emoji, description 2-tuple list\n",
    "    with open(emoji_file) as emojis:\n",
    "        for defn in emojis:\n",
    "            # The file is tab-delim\n",
    "            split = defn.split(\"\\t\")\n",
    "\n",
    "            # Get the emoji and the description from the current line\n",
    "            emoji = split[-1].replace(\"\\n\", \"\")\n",
    "            desc = clean_sentence(split[0], lemma_func, keep_stop_words)\n",
    "\n",
    "            # Add each emoji and embedded description to the list\n",
    "            emoji_embeddings.append((emoji, s2v.embed_sentence(desc)))\n",
    "            \n",
    "    # Return the embeddings\n",
    "    return emoji_embeddings\n",
    "\n",
    "emoji_embeddings = generate_emoji_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def closest_emoji(sent: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Get the closest emoji to the given sentence\n",
    "    \n",
    "    Loop through the list of emoji embeddings and keep track of which one has the\n",
    "    lowest cosine distance from the input sentence's embedding. This is the \"closest\"\n",
    "    emoji. The lru_cache designation means that python will store the last [maxsize]\n",
    "    calls to this function with their return value to reduce computation. This is\n",
    "    cleared after every call to the summary function.\n",
    "    \n",
    "    Args:\n",
    "        sent(List[str]): Sentence to check\n",
    "    Ret:\n",
    "        (Tuple[str, int]) Closest emoji, cosine similarity of emoji\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Embed the sentence using sent2vec \n",
    "    emb = s2v.embed_sentence(sent)\n",
    "\n",
    "    # Start the lowest cosine at higher than it could ever be\n",
    "    lowest_cos = 1_000_000\n",
    "\n",
    "    # The best emoji starts as an empty string placeholder\n",
    "    best_emoji = \"\"\n",
    "\n",
    "    # Loop through the dictionary\n",
    "    for emoji in emoji_embeddings:\n",
    "        # Get the current emoji's embedding\n",
    "        emoji_emb = emoji[1]\n",
    "\n",
    "        # Check the cosine difference between the emoji's embedding and\n",
    "        # the sentence's embedding\n",
    "        curr_cos = cosine(emoji_emb, emb)\n",
    "\n",
    "        # If it lower than the lowest then it is the new best\n",
    "        if curr_cos < lowest_cos:\n",
    "            lowest_cos = curr_cos\n",
    "            best_emoji = emoji[0]\n",
    "\n",
    "    # Return a 2-tuple containing the best emoji and its cosine differnece\n",
    "    return best_emoji, lowest_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Generation and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_of_sent(sent: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Return all possible n-gram combinations of a sentence\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to n-gram-ify\n",
    "    Rets:\n",
    "        (List[List[str]]): List of all possible n-gram combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def combinations_of_sum(sum_to: int, combo: List[int]=None) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Return all possible combinations of ints that sum to some int\n",
    "        \n",
    "        Args:\n",
    "            sum_to(int): The number that all sub-arrays should sum to\n",
    "            combo(List[int]): The current combination of number that the recursive\n",
    "                              algo should subdivide, not needed for first run but used\n",
    "                              in every consequent recursive run of the function\n",
    "        \"\"\"\n",
    "        # Initialize the list for combinations\n",
    "        combos = []\n",
    "        \n",
    "        # If the current combo list is none (first run through)\n",
    "        # then generate it with all 1s and length = sum_to\n",
    "        if combo is None:\n",
    "            combo = [1 for x in range(sum_to)]\n",
    "            combos.append(combo)\n",
    "\n",
    "        # Base case: If the length  of the combination is 0 then\n",
    "        # end the recursion because we are at the top of the \"tree\"\n",
    "        if len(combo) == 0:\n",
    "            return None\n",
    "\n",
    "        # For each \n",
    "        for i in range(1, len(combo)):\n",
    "            combo_to_query = combo[:i-1] + [sum(combo[i - 1:i + 1])] + combo[i+1:]\n",
    "            combos.append(combo_to_query)\n",
    "            [combos.append(combo) for combo in combinations_of_sum(sum_to, combo_to_query) if combo is not None]\n",
    "\n",
    "        return combos\n",
    "    \n",
    "    def combinations_of_sent_helper(sent):\n",
    "        sent = word_tokenize(sent)\n",
    "        combos = np.unique(combinations_of_sum(len(sent)))\n",
    "        sent_combos = []\n",
    "        for combo in combos:\n",
    "            sent_combo = []\n",
    "            curr_i = 0\n",
    "            for combo_len in combo:\n",
    "                space_joined = \" \".join(sent[curr_i:combo_len + curr_i])\n",
    "                if space_joined not in sent_combo:\n",
    "                    sent_combo.append(space_joined) \n",
    "                curr_i += combo_len\n",
    "\n",
    "            if sent_combo not in sent_combos:\n",
    "                sent_combos.append(sent_combo)\n",
    "        return sent_combos\n",
    "    \n",
    "    return combinations_of_sent_helper(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Algorithm and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmojiSummarizationResult:\n",
    "    \"\"\"\n",
    "    \"Struct\" for keeping track of an Emoji Summarization result\n",
    "    \n",
    "    Data Members:\n",
    "        emojis(str): String of emojis that represent the summarization\n",
    "        n_grams(List[str]): List of variable length n-grams that each emoji represents\n",
    "        uncertainty_scores(List[float]): List of the cosine distance between each n_gram and emoji\n",
    "        time_elapsed(float): How long it took to complete the summary\n",
    "    \"\"\"\n",
    "    emojis: str = \"\"\n",
    "    n_grams: List[str] = field(default_factory=list)\n",
    "    uncertainty_scores: List[float] = field(default_factory=list)\n",
    "    elapsed_time: float = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_summarization_result(summarization: EmojiSummarizationResult) -> float:\n",
    "    \"\"\"\n",
    "    Score a EmojiSummarizationResult\n",
    "    \n",
    "    Get the average of all uncertainty scores and return that as the score\n",
    "    \n",
    "    Args:\n",
    "        summarization(EmojiSummarizationResult): Summarization to score\n",
    "        \n",
    "    Rets:\n",
    "        (float): Numerical summarization score\n",
    "    \"\"\"\n",
    "    return sum(summarization.uncertainty_scores) / len(summarization.uncertainty_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sent:str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> EmojiSummarizationResult: \n",
    "    \"\"\"\n",
    "    Summarize the given sentence into emojis\n",
    "    \n",
    "    Split the sentence into every possible combination of n-grams and see which returns the highest score\n",
    "    when each n-gram is translated to an emoji using the closest emoji in the dataset\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to summarize\n",
    "        lemma_func(Callable[[str], str]): Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the cleaned sentence\n",
    "    Rets:\n",
    "        (Tuple[List[str], List[float], List[str]]): (Emoji Sentence, \n",
    "        List of Uncertainty values for the corresponding emoji,\n",
    "        list of n-grams used to generate the corresponding emoji)\n",
    "    \"\"\"\n",
    "    # Clean the sentence\n",
    "    sent = clean_sentence(sent, lemma_func, keep_stop_words)\n",
    "    \n",
    "    # Generate all combinations of sentences\n",
    "    sent_combos = combinations_of_sent(sent)\n",
    "    # Init \"best\" datamembers as empty or exceedingly high\n",
    "    best_summarization = EmojiSummarizationResult()\n",
    "    best_summarization_score = 100_000_000\n",
    "    # Iterate through every combination of sentence combos\n",
    "    for sent_combo in sent_combos:\n",
    "        # Start the local data members as empty\n",
    "        local_summarization = EmojiSummarizationResult()\n",
    "        # Iterate through each n_gram adding the uncertainty and emoji to the lists\n",
    "        for n_gram in sent_combo:\n",
    "            close_emoji, cos_diff = closest_emoji(n_gram)\n",
    "            local_summarization.emojis += close_emoji\n",
    "            local_summarization.uncertainty_scores.append(cos_diff)\n",
    "        \n",
    "        local_summarization.n_grams = sent_combo\n",
    "\n",
    "        # Check if the average uncertainty is less than the best\n",
    "        # TODO: Maybe a median check would be helpful as well?\n",
    "        if score_summarization_result(local_summarization) < best_summarization_score:\n",
    "            # Update the best emojis\n",
    "            best_summarization = local_summarization\n",
    "            best_summarization_score = score_summarization_result(best_summarization)\n",
    "            \n",
    "    # Clear the function cache on closest_emoji because it is unlikely the next run will make use of them\n",
    "    closest_emoji.cache_clear()\n",
    "    \n",
    "    # Return the emoji \"sentence\", list of all the cosine similarities, and all of the n-grams\n",
    "    return best_summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(sents: List[str], lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True, generate_embeddings: bool=True) -> HTML:\n",
    "    \"\"\"\n",
    "    Summarize a collection of sentences and display it nicely with IPython\n",
    "    \n",
    "    Args:\n",
    "        sents(List[str]): List of sentences to translate\n",
    "        lemma_func(Callable[[str], str]), optional: Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool), optional: Keep the stop words in the cleaned sentence\n",
    "        generate_embeddings(bool), optional: Regenerate the emoji embeddings for the case that the lemmatazation/stop_word params have changed\n",
    "        \n",
    "    Rets:\n",
    "        IPython.HTML: HTML List to be displayed with IPython\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate emoji embeddings in case the cleaning parameters have changed\n",
    "    if generate_embeddings:\n",
    "        time_now = time()\n",
    "        global emoji_embeddings\n",
    "        emoji_embeddings = generate_emoji_embeddings(lemma_func, keep_stop_words)\n",
    "        print(\"Completed emoji embeddings, time elapsed: {}\\n\".format(time() - time_now))\n",
    "    \n",
    "    # Create the 2d array for the talbe\n",
    "    table = []\n",
    "    \n",
    "    # Iterate through each sentence to be summarized\n",
    "    for sent in sents:\n",
    "        # Start timer\n",
    "        time_now = time()\n",
    "        \n",
    "        # Summarize it\n",
    "        summarization_res = summarize(sent, lemma_func, keep_stop_words)\n",
    "        \n",
    "        # Get elapsed time\n",
    "        elapsed_time = time() - time_now\n",
    "        \n",
    "        # Update elapsed time\n",
    "        summarization_res.elapsed_time = elapsed_time\n",
    "        \n",
    "        # Print status update\n",
    "        # print(\"Completed sentence: {}, time elapsed: {}\".format(sents.index(sent), elapsed_time))\n",
    "\n",
    "        # Append pertinent data to the table\n",
    "        table.append([sent, round(1 - score_summarization_result(summarization_res), 3), \n",
    "                           [round(1 - x, 3) for x in summarization_res.uncertainty_scores],\n",
    "                           summarization_res.n_grams, \n",
    "                           summarization_res.elapsed_time,\n",
    "                           summarization_res.emojis])\n",
    "        \n",
    "        # Print out an update\n",
    "    \n",
    "    # Return the table with the headers\n",
    "    return tabulate(table, tablefmt='pipe', \n",
    "                                headers=[\"Input Sentence\", \"Summary Score\", \"Individual N-Gram Scores\", \n",
    "                                         \"N-Grams\", \"Elapsed Time\", \"Emoji Results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### No lemmatization, with stop words\n",
      "Completed emoji embeddings, time elapsed: 0.26052331924438477\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores     | N-Grams                                           |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:-----------------------------|:--------------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.44  | [0.199, 0.703, 0.518, 0.34]  | ['we need to rent', 'a', 'room', 'for our party'] |       2.88409  | 🈹💾🚪🥳         |\n",
      "| She folded her handkerchief neatly.     |           0.389 | [0.399, 0.481, 0.287]        | ['she folded', 'her', 'handkerchief neatly']      |       0.829407 | 🥟💆🎀           |\n",
      "| I'd rather be a bird than a fish.       |           0.58  | [0.488, 0.703, 0.55]         | [\"i 'd rather be a bird than\", 'a', 'fish']       |       2.71226  | 🐦💾🐟          |\n",
      "| Tom got a small piece of pie.           |           0.448 | [0.184, 0.703, 0.616, 0.287] | ['tom got', 'a', 'small', 'piece of pie']         |       1.53654  | 💆💾🚣🥧         |\n",
      "| The lake is a long way from here.       |           0.401 | [0.247, 0.703, 0.252]        | ['the lake is', 'a', 'long way from here']        |       2.49323  | 🚤💾🈁          |\n",
      "| Rock music approaches at high velocity. |           0.539 | [0.655, 0.524, 0.439]        | ['rock', 'music', 'approaches at high velocity']  |       1.14726  | 🧗🎵🚄           |\n",
      "\n",
      "### No lemmatization, no stop words\n",
      "Completed emoji embeddings, time elapsed: 0.24212169647216797\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores    | N-Grams                                       |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:----------------------------|:----------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.461 | [0.422, 0.501]              | ['need rent room', 'party']                   |       0.543367 | 🚪🥳             |\n",
      "| She folded her handkerchief neatly.     |           0.399 | [0.486, 0.311]              | ['folded', 'handkerchief neatly']             |       0.325372 | 🥟👝             |\n",
      "| I'd rather be a bird than a fish.       |           0.536 | [0.486, 0.585]              | [\"'d rather bird\", 'fish']                    |       0.53641  | 🐦🐟            |\n",
      "| Tom got a small piece of pie.           |           0.425 | [0.279, 0.644, 0.45, 0.325] | ['tom got', 'small', 'piece', 'pie']          |       0.809613 | 💆🚣📄🍰        |\n",
      "| The lake is a long way from here.       |           0.343 | [0.346, 0.339]              | ['lake', 'long way']                          |       0.322598 | 🚤🥒            |\n",
      "| Rock music approaches at high velocity. |           0.564 | [0.685, 0.557, 0.451]       | ['rock', 'music', 'approaches high velocity'] |       0.808164 | 🧗🎵🚄           |\n",
      "\n",
      "### Wordnet Lemmatizer, with stop words\n",
      "Completed emoji embeddings, time elapsed: 0.30639004707336426\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores     | N-Grams                                           |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:-----------------------------|:--------------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.428 | [0.17, 0.703, 0.381, 0.458]  | ['we need to rent', 'a', 'room for our', 'party'] |       2.7611   | 🈹💾🚪🥳         |\n",
      "| She folded her handkerchief neatly.     |           0.395 | [0.404, 0.481, 0.3]          | ['she folded', 'her', 'handkerchief neatly']      |       0.821405 | 🥟💆👘           |\n",
      "| I'd rather be a bird than a fish.       |           0.582 | [0.488, 0.703, 0.556]        | [\"i 'd rather be a bird than\", 'a', 'fish']       |       2.83004  | 🐦💾🍣          |\n",
      "| Tom got a small piece of pie.           |           0.449 | [0.184, 0.703, 0.616, 0.292] | ['tom got', 'a', 'small', 'piece of pie']         |       1.53374  | 💆💾🚣🥧         |\n",
      "| The lake is a long way from here.       |           0.434 | [0.364, 0.703, 0.237]        | ['the lake is', 'a', 'long way from here']        |       2.01294  | 🚤💾🥾           |\n",
      "| Rock music approaches at high velocity. |           0.565 | [0.655, 0.575, 0.465]        | ['rock', 'music', 'approach at high velocity']    |       1.14433  | 🧗🎵🚄           |\n",
      "\n",
      "### Wordnet Lemmatizer, no stop words\n",
      "Completed emoji embeddings, time elapsed: 0.2833833694458008\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores    | N-Grams                                     |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:----------------------------|:--------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.471 | [0.422, 0.521]              | ['need rent room', 'party']                 |       0.544961 | 🚪🥳             |\n",
      "| She folded her handkerchief neatly.     |           0.399 | [0.486, 0.311]              | ['folded', 'handkerchief neatly']           |       0.329441 | 🥟👝             |\n",
      "| I'd rather be a bird than a fish.       |           0.531 | [0.486, 0.577]              | [\"'d rather bird\", 'fish']                  |       0.541451 | 🐦🍣            |\n",
      "| Tom got a small piece of pie.           |           0.425 | [0.279, 0.644, 0.45, 0.325] | ['tom got', 'small', 'piece', 'pie']        |       0.809405 | 💆🚣📄🍰        |\n",
      "| The lake is a long way from here.       |           0.429 | [0.519, 0.339]              | ['lake', 'long way']                        |       0.340311 | 🚤🥒            |\n",
      "| Rock music approaches at high velocity. |           0.594 | [0.685, 0.612, 0.484]       | ['rock', 'music', 'approach high velocity'] |       0.804777 | 🧗🎵🚄           |\n",
      "\n",
      "### Spacy Lemmatizer, with stop words\n",
      "Completed emoji embeddings, time elapsed: 30.82019877433777\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores     | N-Grams                                                  |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:-----------------------------|:---------------------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.443 | [0.163, 0.703, 0.458, 0.449] | ['-PRON- need to rent', 'a', 'room for', '-PRON- party'] |       2.85166  | 🚎💾🚪🥳         |\n",
      "| She folded her handkerchief neatly.     |           0.353 | [0.353]                      | ['-PRON- fold -PRON- handkerchief neatly']               |       0.788352 | 🥡               |\n",
      "| I'd rather be a bird than a fish.       |           0.569 | [0.461, 0.703, 0.542]        | ['i have rather be a bird than', 'a', 'fish']            |       2.89151  | 🥀💾🎣          |\n",
      "| Tom got a small piece of pie.           |           0.492 | [0.331, 0.703, 0.616, 0.32]  | ['tom get', 'a', 'small', 'piece of pie']                |       1.55225  | 💆💾🚣🍞        |\n",
      "| The lake is a long way from here.       |           0.516 | [0.437, 0.651, 0.703, 0.275] | ['the lake', 'be', 'a', 'long way from here']            |       2.01716  | 🚤🧞💾🥕         |\n",
      "| Rock music approaches at high velocity. |           0.572 | [0.67, 0.572, 0.475]         | ['rock', 'music', 'approaches at high velocity']         |       1.17187  | 🧗🎵🚄           |\n",
      "\n",
      "### Spacy Lemmatizer, no stop words\n",
      "Completed emoji embeddings, time elapsed: 18.736726999282837\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores   | N-Grams                                       |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:---------------------------|:----------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.454 | [0.422, 0.486]             | ['need rent room', 'party']                   |       0.555363 | 🚪🥳             |\n",
      "| She folded her handkerchief neatly.     |           0.372 | [0.372]                    | ['fold handkerchief neatly']                  |       0.33495  | 🥡               |\n",
      "| I'd rather be a bird than a fish.       |           0.527 | [0.479, 0.575]             | ['have rather bird', 'fish']                  |       0.55914  | 🐦🐟            |\n",
      "| Tom got a small piece of pie.           |           0.453 | [0.367, 0.644, 0.35]       | ['tom get', 'small', 'piece pie']             |       0.826017 | 💆🚣🍞          |\n",
      "| The lake is a long way from here.       |           0.439 | [0.538, 0.339]             | ['lake', 'long way']                          |       0.333523 | 🚤🥒            |\n",
      "| Rock music approaches at high velocity. |           0.587 | [0.691, 0.59, 0.478]       | ['rock', 'music', 'approaches high velocity'] |       0.821203 | 🧗🎵🚄           |\n",
      "\n",
      "### Porter Stemmer, with stop words\n",
      "Completed emoji embeddings, time elapsed: 0.46215343475341797\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores            | N-Grams                                     |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:------------------------------------|:--------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.46  | [0.367, 0.552]                      | ['we need to rent a room for our', 'parti'] |       2.80985  | 🚪🥳             |\n",
      "| She folded her handkerchief neatly.     |           0.373 | [0.379, 0.367]                      | ['she fold', 'her handkerchief neatli']     |       0.822813 | 🥡🎀             |\n",
      "| I'd rather be a bird than a fish.       |           0.645 | [0.577, 0.703, 0.656]               | [\"i 'd rather be a bird than\", 'a', 'fish'] |       2.84725  | 🐦💾🎣          |\n",
      "| Tom got a small piece of pie.           |           0.472 | [0.197, 0.703, 0.656, 0.525, 0.281] | ['tom got', 'a', 'small piec', 'of', 'pie'] |       1.54378  | 💆💾🚣💊🍞      |\n",
      "| The lake is a long way from here.       |           0.465 | [0.404, 0.703, 0.288]               | ['the lake is', 'a', 'long way from here']  |       2.00287  | 🚤💾🥕          |\n",
      "| Rock music approaches at high velocity. |           0.558 | [0.651, 0.643, 0.379]               | ['rock', 'music', 'approach at high veloc'] |       1.15525  | 🧗🎵🚄           |\n",
      "\n",
      "### Porter Stemmer, no stop words\n",
      "Completed emoji embeddings, time elapsed: 0.42127180099487305\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores   | N-Grams                                  |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:---------------------------|:-----------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.503 | [0.424, 0.582]             | ['need rent room', 'parti']              |       0.551874 | 🚪👗            |\n",
      "| She folded her handkerchief neatly.     |           0.424 | [0.424]                    | ['fold handkerchief neatli']             |       0.348933 | 🥡               |\n",
      "| I'd rather be a bird than a fish.       |           0.627 | [0.561, 0.693]             | [\"'d rather bird\", 'fish']               |       0.539492 | 🐦🎣            |\n",
      "| Tom got a small piece of pie.           |           0.436 | [0.268, 0.737, 0.304]      | ['tom got', 'small', 'piec pie']         |       0.825557 | 💆🚣🥪           |\n",
      "| The lake is a long way from here.       |           0.491 | [0.572, 0.41]              | ['lake', 'long way']                     |       0.33101  | 🚤🥒            |\n",
      "| Rock music approaches at high velocity. |           0.592 | [0.675, 0.703, 0.397]      | ['rock', 'music', 'approach high veloc'] |       0.831188 | 🧗🎵🚄           |\n",
      "\n",
      "### Snowball Stemmer, with stop words\n",
      "Completed emoji embeddings, time elapsed: 0.39820289611816406\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores            | N-Grams                                     |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:------------------------------------|:--------------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.46  | [0.367, 0.552]                      | ['we need to rent a room for our', 'parti'] |       2.94195  | 🚪🥳             |\n",
      "| She folded her handkerchief neatly.     |           0.399 | [0.379, 0.492, 0.327]               | ['she fold', 'her', 'handkerchief neat']    |       0.821262 | 🥡💆🧶            |\n",
      "| I'd rather be a bird than a fish.       |           0.616 | [0.488, 0.703, 0.656]               | [\"i 'd rather be a bird than\", 'a', 'fish'] |       2.88236  | 🐦💾🎣          |\n",
      "| Tom got a small piece of pie.           |           0.468 | [0.175, 0.703, 0.656, 0.525, 0.281] | ['tom got', 'a', 'small piec', 'of', 'pie'] |       1.55203  | 💆💾🚣💊🍞      |\n",
      "| The lake is a long way from here.       |           0.465 | [0.404, 0.703, 0.288]               | ['the lake is', 'a', 'long way from here']  |       2.05877  | 🚤💾🥕          |\n",
      "| Rock music approaches at high velocity. |           0.558 | [0.651, 0.643, 0.379]               | ['rock', 'music', 'approach at high veloc'] |       1.16155  | 🧗🎵🚄           |\n",
      "\n",
      "### Snowball Stemmer, no stop words\n",
      "Completed emoji embeddings, time elapsed: 0.3679392337799072\n",
      "\n",
      "| Input Sentence                          |   Summary Score | Individual N-Gram Scores   | N-Grams                                  |   Elapsed Time | Emoji Results   |\n",
      "|:----------------------------------------|----------------:|:---------------------------|:-----------------------------------------|---------------:|:----------------|\n",
      "| We need to rent a room for our party.   |           0.503 | [0.424, 0.582]             | ['need rent room', 'parti']              |       0.55168  | 🚪👗            |\n",
      "| She folded her handkerchief neatly.     |           0.431 | [0.431]                    | ['fold handkerchief neat']               |       0.327883 | 🥡               |\n",
      "| I'd rather be a bird than a fish.       |           0.59  | [0.486, 0.693]             | [\"'d rather bird\", 'fish']               |       0.544478 | 🐦🎣            |\n",
      "| Tom got a small piece of pie.           |           0.436 | [0.268, 0.737, 0.304]      | ['tom got', 'small', 'piec pie']         |       0.824745 | 💆🚣🥪           |\n",
      "| The lake is a long way from here.       |           0.491 | [0.572, 0.41]              | ['lake', 'long way']                     |       0.330163 | 🚤🥒            |\n",
      "| Rock music approaches at high velocity. |           0.592 | [0.675, 0.703, 0.397]      | ['rock', 'music', 'approach high veloc'] |       0.821996 | 🧗🎵🚄           |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testing_sentences = [\"We need to rent a room for our party.\", \"She folded her handkerchief neatly.\", \"I'd rather be a bird than a fish.\", \"Tom got a small piece of pie.\", \"The lake is a long way from here.\", \"Rock music approaches at high velocity.\"]\n",
    "lemmatization_functions = [(lambda x: x, \"No lemmatization\"),\n",
    "                           (lemmatizerNLTK.lemmatize, \"Wordnet Lemmatizer\"),\n",
    "                           (lambda x: lemmatizerSpacy(x)[0].lemma_, \"Spacy Lemmatizer\"),\n",
    "                           (ps.stem, \"Porter Stemmer\"),\n",
    "                           (sb.stem, \"Snowball Stemmer\")]\n",
    "for lemmatization_function, desc in lemmatization_functions:\n",
    "    for stop_words in [True, False]:\n",
    "        print(\"### {}, {}\".format(desc, \"with stop words\" if stop_words else \"no stop words\"))\n",
    "        print(format_summary(testing_sentences, lemmatization_function, stop_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
