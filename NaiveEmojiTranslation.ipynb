{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Sentence to Emoji Translation \n",
    "## Purpose\n",
    "To workshop a naive version of an sentence to emoji translation algorithm. The general idea is that sentences can be \"chuncked\" out into n-grams that are more related to a single emoji. The related-ness of an n-gram to an emoji is directly related to the cosine similarity of the sent2vec representation of the sentence and the sent2vec representation of one of the emoji's definitions. The emoji definitons are gathered from the [emoji2vec](https://github.com/uclmr/emoji2vec) github repo and the sent2vec model is from the [sent2vec](https://github.com/epfml/sent2vec) github repo. \n",
    "\n",
    "## Issues\n",
    "- The generation of the summary is so incredibly slow\n",
    "- There are some issues with lemmatization (e.g. poop != pooped when lemmatized)\n",
    "- /opt/conda/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
    "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
    "\n",
    "## Ideas\n",
    "- Add bias for fewer emojis. Some of the generated sentences are just the sentence translated into 1-grams and  it is really easy to find an emoji that represents a one word. If some how the sentence was scored both based on sum similarity and the length of the sentence that might produce better summarizations\n",
    "    \n",
    "- Turn the summarization into a class as to easily test new configurations of lemmatizers/stop-words. \n",
    "\n",
    "## Notes\n",
    "Wikipedia dataset from https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List, Tuple, Callable\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from functools import lru_cache\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import spacy\n",
    "from dataclasses import dataclass, field\n",
    "import tabulate\n",
    "from IPython.display import display, HTML\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_file = \"./data/emoji_joined.txt\"\n",
    "wikipedia_file = \"./data/wikipedia_utf8_filtered_20pageviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sent2vec model\n",
    "s2v = sent2vec.Sent2vecModel()\n",
    "s2v.load_model('../models/wiki_unigrams.bin') # https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmojiSummarizationResult:\n",
    "    \"Class for keeping track of an emoji summarization result\"\n",
    "    emojis: str = \"\"\n",
    "    n_grams: List[str] = field(default_factory=list)\n",
    "    uncertainty_scores: List[float] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize the NLTK lemmatizer\n",
    "lemmatizerSpacy = spacy.load('en', disable=['parser', 'ner'])\n",
    "ps = PorterStemmer()\n",
    "sb = SnowballStemmer(\"english\")\n",
    "lemmatizerNLTK = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Cleaning\n",
    "The general idea with sentence cleaning is that the sentences need to be put into the same \"format\" for better analysis. There are two main aspects of cleaning: 1) removal, and 2) modification. Removal is primarily for tokens that do not contribute to the sentence at all. These include \".\", \"and\", \"but\". Normally this is a standard step in sentence cleaning but it has actually has zero effect on the output that I can see. However, token modification changes the spelling of tokens to uniform all tokens that use the same root. For example \"rocked\", \"rock\", \"rocking\" should all be reduced to their lemma of \"rock\". There are two different ways to do this: [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent: str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Clean a sentence\n",
    "    \n",
    "    Tokenize the word and then lemmatize each individual word before rejoining it all together.\n",
    "    Optionally removing stop words along the way\n",
    "        \n",
    "    Args:\n",
    "        sent(str): Sentence to clean\n",
    "        lemma_func(Callable[[str], str]): A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the sentence\n",
    "    Rets:\n",
    "        (str): Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Lemmatize each word in the sentence\n",
    "    #return \" \".join([token.lemma_ for token in lemmatizer(sent.lower())])\n",
    "    return \" \".join([lemma_func(token) for token in word_tokenize(sent.lower()) if token not in stopwords or keep_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array to store the (emoji, repr) 2-tuple\n",
    "def generate_emoji_embeddings(lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Generate the sent2vec emoji embeddings from the input file\n",
    "    \n",
    "    Run each emoji within the emoji_joined data file from the emoji2vec paper through\n",
    "    the sent2vec sentence embedder. This is a very naive way of doing it because one\n",
    "    emoji may have multiple entries in the data file so it has multiple vectors in the\n",
    "    emoji_embeddings array\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    Rets:\n",
    "        (List[Tuple[str, List[float]]]): A list of 2-tuples containing the emoji and \n",
    "                                         one vector representation of it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list that will hold all of the embedings\n",
    "    emoji_embeddings = []\n",
    "    \n",
    "    # Open the file that stores the emoji, description 2-tuple list\n",
    "    with open(emoji_file) as emojis:\n",
    "        for defn in emojis:\n",
    "            # The file is tab-delim\n",
    "            split = defn.split(\"\\t\")\n",
    "\n",
    "            # Get the emoji and the description from the current line\n",
    "            emoji = split[-1].replace(\"\\n\", \"\")\n",
    "            desc = clean_sentence(split[0], lemma_func, keep_stop_words)\n",
    "\n",
    "            # Add each emoji and embedded description to the list\n",
    "            emoji_embeddings.append((emoji, s2v.embed_sentence(desc)))\n",
    "            \n",
    "    # Return the embeddings\n",
    "    return emoji_embeddings\n",
    "\n",
    "emoji_embeddings = generate_emoji_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def closest_emoji(sent: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Get the closest emoji to the given sentence\n",
    "    \n",
    "    Loop through the list of emoji embeddings and keep track of which one has the\n",
    "    lowest cosine distance from the input sentence's embedding. This is the \"closest\"\n",
    "    emoji. The lru_cache designation means that python will store the last [maxsize]\n",
    "    calls to this function with their return value to reduce computation. This is\n",
    "    cleared after every call to the summary function.\n",
    "    \n",
    "    Args:\n",
    "        sent(List[str]): Sentence to check\n",
    "    Ret:\n",
    "        (Tuple[str, int]) Closest emoji, cosine similarity of emoji\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Embed the sentence using sent2vec \n",
    "    emb = s2v.embed_sentence(sent)\n",
    "\n",
    "    # Start the lowest cosine at higher than it could ever be\n",
    "    lowest_cos = 1_000_000\n",
    "\n",
    "    # The best emoji starts as an empty string placeholder\n",
    "    best_emoji = \"\"\n",
    "\n",
    "    # Loop through the dictionary\n",
    "    for emoji in emoji_embeddings:\n",
    "        # Get the current emoji's embedding\n",
    "        emoji_emb = emoji[1]\n",
    "\n",
    "        # Check the cosine difference between the emoji's embedding and\n",
    "        # the sentence's embedding\n",
    "        curr_cos = cosine(emoji_emb, emb)\n",
    "\n",
    "        # If it lower than the lowest then it is the new best\n",
    "        if curr_cos < lowest_cos:\n",
    "            lowest_cos = curr_cos\n",
    "            best_emoji = emoji[0]\n",
    "\n",
    "    # Return a 2-tuple containing the best emoji and its cosine differnece\n",
    "    return best_emoji, lowest_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_of_sent(sent: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Return all possible n-gram combinations of a sentence\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to n-gram-ify\n",
    "    Rets:\n",
    "        (List[List[str]]): List of all possible n-gram combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def combinations_of_sum(sum_to: int, combo: List[int]=None) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Return all possible combinations of ints that sum to some int\n",
    "        \n",
    "        Args:\n",
    "            sum_to(int): The number that all sub-arrays should sum to\n",
    "            combo(List[int]): The current combination of number that the recursive\n",
    "                              algo should subdivide, not needed for first run but used\n",
    "                              in every consequent recursive run of the function\n",
    "        \"\"\"\n",
    "        # Initialize the list for combinations\n",
    "        combos = []\n",
    "        \n",
    "        # If the current combo list is none (first run through)\n",
    "        # then generate it with all 1s and length = sum_to\n",
    "        if combo is None:\n",
    "            combo = [1 for x in range(sum_to)]\n",
    "            combos.append(combo)\n",
    "\n",
    "        # Base case: If the length  of the combination is 0 then\n",
    "        # end the recursion because we are at the top of the \"tree\"\n",
    "        if len(combo) == 0:\n",
    "            return None\n",
    "\n",
    "        # For each \n",
    "        for i in range(1, len(combo)):\n",
    "            combo_to_query = combo[:i-1] + [sum(combo[i - 1:i + 1])] + combo[i+1:]\n",
    "            combos.append(combo_to_query)\n",
    "            [combos.append(combo) for combo in combinations_of_sum(sum_to, combo_to_query) if combo is not None]\n",
    "\n",
    "        return combos\n",
    "    \n",
    "    def combinations_of_sent_helper(sent):\n",
    "        sent = word_tokenize(sent)\n",
    "        combos = np.unique(combinations_of_sum(len(sent)))\n",
    "        sent_combos = []\n",
    "        for combo in combos:\n",
    "            sent_combo = []\n",
    "            curr_i = 0\n",
    "            for combo_len in combo:\n",
    "                space_joined = \" \".join(sent[curr_i:combo_len + curr_i])\n",
    "                if space_joined not in sent_combo:\n",
    "                    sent_combo.append(space_joined) \n",
    "                curr_i += combo_len\n",
    "\n",
    "            if sent_combo not in sent_combos:\n",
    "                sent_combos.append(sent_combo)\n",
    "        return sent_combos\n",
    "    \n",
    "    return combinations_of_sent_helper(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_summarization_result(summarization: EmojiSummarizationResult) -> float:\n",
    "    return sum(summarization.uncertainty_scores) / len(summarization.uncertainty_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sent:str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> EmojiSummarizationResult: \n",
    "    \"\"\"\n",
    "    Summarize the given sentence into emojis\n",
    "    \n",
    "    Split the sentence into every possible combination of n-grams and see which returns the highest score\n",
    "    when each n-gram is translated to an emoji using the closest emoji in the dataset\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to summarize\n",
    "    Rets:\n",
    "        (Tuple[List[str], List[float], List[str]]): (Emoji Sentence, \n",
    "        List of Uncertainty values for the corresponding emoji,\n",
    "        list of n-grams used to generate the corresponding emoji)\n",
    "    \"\"\"\n",
    "    # Clean the sentence\n",
    "    sent = clean_sentence(sent, lemma_func, keep_stop_words)\n",
    "    \n",
    "    # Generate all combinations of sentences\n",
    "    sent_combos = combinations_of_sent(sent)\n",
    "    # Init \"best\" datamembers as empty or exceedingly high\n",
    "    best_summarization = EmojiSummarizationResult()\n",
    "    best_summarization_score = 100_000_000\n",
    "    # Iterate through every combination of sentence combos\n",
    "    for sent_combo in sent_combos:\n",
    "        # Start the local data members as empty\n",
    "        local_summarization = EmojiSummarizationResult()\n",
    "        # Iterate through each n_gram adding the uncertainty and emoji to the lists\n",
    "        for n_gram in sent_combo:\n",
    "            close_emoji, cos_diff = closest_emoji(n_gram)\n",
    "            local_summarization.emojis += close_emoji\n",
    "            local_summarization.uncertainty_scores.append(cos_diff)\n",
    "        \n",
    "        local_summarization.n_grams = sent_combo\n",
    "\n",
    "        # Check if the average uncertainty is less than the best\n",
    "        # TODO: Maybe a median check would be helpful as well?\n",
    "        if score_summarization_result(local_summarization) < best_summarization_score:\n",
    "            # Update the best emojis\n",
    "            best_summarization = local_summarization\n",
    "            best_summarization_score = score_summarization_result(best_summarization)\n",
    "            \n",
    "    # Clear the function cache on closest_emoji because it is unlikely the next run will make use of them\n",
    "    closest_emoji.cache_clear()\n",
    "    \n",
    "    # Return the emoji \"sentence\", list of all the cosine similarities, and all of the n-grams\n",
    "    return best_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(sents: List[str], lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> HTML:\n",
    "    \"\"\"\n",
    "    Summarize a collection of sentences and display it nicely with IPython\n",
    "    \n",
    "    Args:\n",
    "        sents(List[str]): List of sentences to translate\n",
    "        \n",
    "    Rets:\n",
    "        IPython.HTML: HTML List to be displayed with IPython\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate emoji embeddings in case the cleaning parameters have changed\n",
    "    emoji_embeddings = generate_emoji_embeddings(lemma_func, keep_stop_words)\n",
    "    \n",
    "    # Create the 2d array for the talbe\n",
    "    table = []\n",
    "    \n",
    "    # Iterate through each sentence to be summarized\n",
    "    for sent in sents:\n",
    "        # Summarize it\n",
    "        summarization_res = summarize(sent, lemma_func, keep_stop_words)\n",
    "        # Append pertinent data to the table\n",
    "        table.append([sent, round(1 - score_summarization_result(summarization_res), 3), \n",
    "                           [round(1 - x, 3) for x in summarization_res.uncertainty_scores],\n",
    "                           summarization_res.n_grams, \n",
    "                           summarization_res.emojis])\n",
    "        \n",
    "    # Return the table with the headers\n",
    "    return HTML(tabulate.tabulate(table, tablefmt='html', \n",
    "                                headers=[\"<b>Input Sentence</b>\", \"<b>Summary Score</b>\", \"<b>Individual N-Gram Scores</b>\", \n",
    "                                         \"<b>N-Grams</b>\", \"<b>Emoji Results</b>\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sentences = [\"christmas music rings from the clock tower\", \"It not perfect but it is a start\", \"The sun is rising over new york city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>No lemmatization, with stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                      </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.943</td><td>[1.0, 1.0, 0.829]                </td><td>['christmas', 'music', 'rings from the clock tower']</td><td>🎄🎻🏫                </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 0.879</td><td>[0.838, 0.677, 1.0, 1.0]         </td><td>['it not', 'perfect but it is', 'a', 'start']       </td><td>🙅💯💯🌱              </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.881</td><td>[0.644, 1.0, 1.0]                </td><td>['the sun is rising over', 'new york', 'city']      </td><td>🌄🗽🚏                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>No lemmatization, no stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                             </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.956</td><td>[1.0, 1.0, 0.867]                </td><td>['christmas', 'music', 'rings clock tower']</td><td>🎄🎻🏫                </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0]                       </td><td>['perfect', 'start']                       </td><td>💯🌱                  </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.927</td><td>[0.781, 1.0, 1.0]                </td><td>['sun rising', 'new york', 'city']         </td><td>🌄🗽🚏                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Wordnet Lemmatizer, with stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                        </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.983</td><td>[1.0, 1.0, 1.0, 0.934]           </td><td>['christmas', 'music', 'ring', 'from the clock tower']</td><td>🎄🎻💍🏫              </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 0.879</td><td>[0.838, 0.677, 1.0, 1.0]         </td><td>['it not', 'perfect but it is', 'a', 'start']         </td><td>🙅💯💯🌱              </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.881</td><td>[0.644, 1.0, 1.0]                </td><td>['the sun is rising over', 'new york', 'city']        </td><td>🌄🗽🚏                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Wordnet Lemmatizer, no stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                  </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0, 1.0, 1.0, 1.0]        </td><td>['christmas', 'music', 'ring', 'clock', 'tower']</td><td>🎄🎻💍⏰🏰            </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0]                       </td><td>['perfect', 'start']                            </td><td>💯🌱                  </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.927</td><td>[0.781, 1.0, 1.0]                </td><td>['sun rising', 'new york', 'city']              </td><td>🌄🗽🚏                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Spacy Lemmatizer, with stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                        </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.983</td><td>[1.0, 1.0, 1.0, 0.934]           </td><td>['christmas', 'music', 'ring', 'from the clock tower']</td><td>🎄🎻💍🏫              </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 0.901</td><td>[1.0, 0.605, 1.0, 1.0]           </td><td>['-PRON- not', 'perfect but -PRON- be', 'a', 'start'] </td><td>🙅💯💯🌱              </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.918</td><td>[0.956, 0.717, 1.0, 1.0]         </td><td>['the sun', 'be rise over', 'new york', 'city']       </td><td>🌄🌇🗽🚏              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Spacy Lemmatizer, no stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                  </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                     1</td><td>[1.0, 1.0, 1.0, 1.0, 1.0]        </td><td>['christmas', 'music', 'ring', 'clock', 'tower']</td><td>🎄🎻💍⏰🏰            </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                     1</td><td>[1.0, 1.0]                       </td><td>['perfect', 'start']                            </td><td>💯🌱                  </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                     1</td><td>[1.0, 1.0, 1.0, 1.0]             </td><td>['sun', 'rise', 'new york', 'city']             </td><td>🌄🌇🗽🚏              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Porter Stemmer, with stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                    </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.978</td><td>[1.0, 1.0, 0.934]                </td><td>['christma music', 'ring', 'from the clock tower']</td><td>🎻💍🏫                </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 0.879</td><td>[0.838, 0.677, 1.0, 1.0]         </td><td>['it not', 'perfect but it is', 'a', 'start']     </td><td>🙅💯💯🌱              </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.849</td><td>[0.836, 1.0, 0.711]              </td><td>['the sun is', 'rise', 'over new york citi']      </td><td>🌄🌇🗽                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Porter Stemmer, no stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                              </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0, 1.0, 1.0]             </td><td>['christma music', 'ring', 'clock', 'tower']</td><td>🎻💍⏰🏰              </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0]                       </td><td>['perfect', 'start']                        </td><td>💯🌱                  </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.926</td><td>[1.0, 1.0, 1.0, 0.704]           </td><td>['sun', 'rise', 'new', 'york citi']         </td><td>🌄🌇🌑🗽              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Snowball Stemmer, with stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                                    </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 0.978</td><td>[1.0, 1.0, 0.934]                </td><td>['christma music', 'ring', 'from the clock tower']</td><td>🎻💍🏫                </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 0.879</td><td>[0.838, 0.677, 1.0, 1.0]         </td><td>['it not', 'perfect but it is', 'a', 'start']     </td><td>🙅💯💯🌱              </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.849</td><td>[0.836, 1.0, 0.711]              </td><td>['the sun is', 'rise', 'over new york citi']      </td><td>🌄🌇🗽                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Snowball Stemmer, no stop words<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th><b>Input Sentence</b>                     </th><th style=\"text-align: right;\">  <b>Summary Score</b></th><th><b>Individual N-Gram Scores</b>  </th><th><b>N-Grams</b>                              </th><th><b>Emoji Results</b>  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>christmas music rings from the clock tower</td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0, 1.0, 1.0]             </td><td>['christma music', 'ring', 'clock', 'tower']</td><td>🎻💍⏰🏰              </td></tr>\n",
       "<tr><td>It not perfect but it is a start          </td><td style=\"text-align: right;\">                 1    </td><td>[1.0, 1.0]                       </td><td>['perfect', 'start']                        </td><td>💯🌱                  </td></tr>\n",
       "<tr><td>The sun is rising over new york city      </td><td style=\"text-align: right;\">                 0.926</td><td>[1.0, 1.0, 1.0, 0.704]           </td><td>['sun', 'rise', 'new', 'york citi']         </td><td>🌄🌇🌑🗽              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatization_functions = [(lambda x: x, \"No lemmatization\"), \n",
    "                           (lemmatizerNLTK.lemmatize, \"Wordnet Lemmatizer\"),\n",
    "                           (lambda x: lemmatizerSpacy(x)[0].lemma_, \"Spacy Lemmatizer\"),\n",
    "                           (ps.stem, \"Porter Stemmer\"),\n",
    "                           (sb.stem, \"Snowball Stemmer\")]\n",
    "for lemmatization_function, desc in lemmatization_functions:\n",
    "    for stop_words in [True, False]:\n",
    "        display(HTML(\"<h4>{}, {}<h4>\".format(desc, \"with stop words\" if stop_words else \"no stop words\")))\n",
    "        display(format_summary(testing_sentences, lemmatization_function, stop_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
