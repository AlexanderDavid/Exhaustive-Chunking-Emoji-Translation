{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Sentence to Emoji Translation \n",
    "## Purpose\n",
    "To workshop a naive version of an sentence to emoji translation algorithm. The general idea is that sentences can be \"chuncked\" out into n-grams that are more related to a single emoji. The related-ness of an n-gram to an emoji is directly related to the cosine similarity of the sent2vec representation of the sentence and the sent2vec representation of one of the emoji's definitions. The emoji definitons are gathered from the [emoji2vec](https://github.com/uclmr/emoji2vec) github repo and the sent2vec model is from the [sent2vec](https://github.com/epfml/sent2vec) github repo. \n",
    "\n",
    "## Issues\n",
    "- The generation of all of the n-grams is so incredibly slow\n",
    "- There are some issues with lemmatization (e.g. poop != pooped when lemmatized\n",
    "- /opt/conda/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
    "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
    "\n",
    "## Ideas\n",
    "- Add bias for fewer emojis. Some of the generated sentences are just the sentence translated into 1-grams and  it is really easy to find an emoji that represents a one word. If some how the sentence was scored both based on sum similarity and the length of the sentence that might produce better summarizations\n",
    "    \n",
    "- Turn the summarization into a class as to easily test new configurations of lemmatizers/stop-words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List, Tuple\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sent2vec model\n",
    "s2v = sent2vec.Sent2vecModel()\n",
    "s2v.load_model('../models/wiki_unigrams.bin') # https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize the NLTK lemmatizer\n",
    "lemmatizerSpacy = spacy.load('en', disable=['parser', 'ner'])\n",
    "ps = PorterStemmer()\n",
    "sb = SnowballStemmer(\"english\")\n",
    "lemmatizerNLTK = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Cleaning\n",
    "The general idea with sentence cleaning is that the sentences need to be put into the same \"format\" for better analysis. There are two main aspects of cleaning: 1) removal, and 2) modification. Removal is primarily for tokens that do not contribute to the sentence at all. These include \".\", \"and\", \"but\". Normally this is a standard step in sentence cleaning but it has actually has zero effect on the output that I can see. However, token modification changes the spelling of tokens to uniform all tokens that use the same root. For example \"rocked\", \"rock\", \"rocking\" should all be reduced to their lemma of \"rock\". There are two different ways to do this: [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_sentence(sent: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and lemmatize a sentence\n",
    "    \n",
    "    TODO: More complex cleaning when the dataset get's more messy\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to clean\n",
    "    Rets:\n",
    "        (str): Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Lemmatize each word in the sentence\n",
    "    #return \" \".join([token.lemma_ for token in lemmatizer(sent.lower())])\n",
    "    return \" \".join([lemma_func(token) for token in word_tokenize(sent.lower()) if token not in stopwords or keep_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array to store the (emoji, repr) 2-tuple\n",
    "emoji_embeddings = []\n",
    "def generate_emoji_embeddings():\n",
    "    global emoji_embeddings\n",
    "    emoji_embeddings = []\n",
    "    # Open the file that stores the emoji, description 2-tuple list\n",
    "    with open(\"emoji_joined.txt\") as emojis:\n",
    "        for defn in emojis:\n",
    "            # The file is tab-delim\n",
    "            split = defn.split(\"\\t\")\n",
    "\n",
    "            # Get the emoji and the description from the current line\n",
    "            emoji = split[-1].replace(\"\\n\", \"\")\n",
    "            desc = clean_sentence(split[0])\n",
    "\n",
    "            # Add each emoji and embedded description to the list\n",
    "            emoji_embeddings.append((emoji, s2v.embed_sentence(desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=100)\n",
    "def closest_emoji(sent: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Get the closest emoji to the given sentence\n",
    "    \n",
    "    Args:\n",
    "        sent(List[str]): Sentence to check\n",
    "    Ret:\n",
    "        (Tuple[str, int]) Closest emoji, the respective cosine similarity\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Embed the sentence using sent2vec \n",
    "    emb = s2v.embed_sentence(sent)\n",
    "\n",
    "    # Start the lowest cosine at higher than it could ever be\n",
    "    lowest_cos = 1_000_000\n",
    "\n",
    "    # The best emoji starts as an empty string placeholder\n",
    "    best_emoji = \"\"\n",
    "\n",
    "    # Loop through the dictionary\n",
    "    for emoji in emoji_embeddings:\n",
    "        # Get the current emoji's embedding\n",
    "        emoji_emb = emoji[1]\n",
    "\n",
    "        # Check the cosine difference between the emoji's embedding and\n",
    "        # the sentence's embedding\n",
    "        curr_cos = cosine(emoji_emb, emb)\n",
    "\n",
    "        # If it lower than the lowest then it is the new best\n",
    "        if curr_cos < lowest_cos:\n",
    "            lowest_cos = curr_cos\n",
    "            best_emoji = emoji[0]\n",
    "\n",
    "    # Return a 2-tuple containing the best emoji and its cosine differnece\n",
    "    return best_emoji, lowest_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_of_sum(sum_to, combo=None):\n",
    "    combos = []\n",
    "    if combo is None:\n",
    "        combo = [1 for x in range(sum_to)]\n",
    "        combos.append(combo)\n",
    "    \n",
    "    if len(combo) == 0:\n",
    "        return None\n",
    "    \n",
    "    for i in range(1, len(combo)):\n",
    "        combo_to_query = combo[:i-1] + [sum(combo[i - 1:i + 1])] + combo[i+1:]\n",
    "        combos.append(combo_to_query)\n",
    "        [combos.append(combo) for combo in combinations_of_sum(sum_to, combo_to_query) if combo is not None]\n",
    "            \n",
    "    return combos\n",
    "    \n",
    "def combinations_of_sent(sent):\n",
    "    sent_combos = []\n",
    "    def combinations_of_sent_helper(sent):\n",
    "        sent = word_tokenize(sent)\n",
    "        combos = np.unique(combinations_of_sum(len(sent)))\n",
    "        sent_combos = []\n",
    "        for combo in combos:\n",
    "            sent_combo = []\n",
    "            curr_i = 0\n",
    "            for combo_len in combo:\n",
    "                space_joined = \" \".join(sent[curr_i:combo_len + curr_i])\n",
    "                if space_joined not in sent_combo:\n",
    "                    sent_combo.append(space_joined) \n",
    "                curr_i += combo_len\n",
    "\n",
    "            if sent_combo not in sent_combos:\n",
    "                sent_combos.append(sent_combo)\n",
    "        return sent_combos\n",
    "    \n",
    "    return combinations_of_sent_helper(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sent:str) -> Tuple[List[str], List[float], List[str]]: \n",
    "    \"\"\"\n",
    "    Summarize the given sentence into emojis\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to summarize\n",
    "    Rets:\n",
    "        (Tuple[List[str], List[float], List[str]]): (Emoji Sentence, \n",
    "        List of Uncertainty values for the corresponding emoji,\n",
    "        list of n-grams used to generate the corresponding emoji)\n",
    "    \"\"\"\n",
    "    # Clean the sentence\n",
    "    sent = clean_sentence(sent)\n",
    "    \n",
    "    # Generate all combinations of sentences\n",
    "    sent_combos = combinations_of_sent(sent)\n",
    "    # Init \"best\" datamembers as empty or exceedingly high\n",
    "    best_emojis = \"\"\n",
    "    best_n_grams = []\n",
    "    best_uncertainties = [100_000_000]\n",
    "    # Iterate through every combination of sentence combos\n",
    "    for sent_combo in sent_combos:\n",
    "        # Start the local data members as empty\n",
    "        emojis = \"\"\n",
    "        uncertainties = []\n",
    "        # Iterate through each n_gram adding the uncertainty and emoji to the lists\n",
    "        for n_gram in sent_combo:\n",
    "            close_emoji, cos_diff = closest_emoji(n_gram)\n",
    "            emojis += close_emoji\n",
    "            uncertainties.append(cos_diff)\n",
    "\n",
    "        # Check if the average uncertainty is less than the best\n",
    "        # TODO: Maybe a median check would be helpful as well?\n",
    "        if sum(uncertainties)/len(uncertainties) < sum(best_uncertainties)/len(best_uncertainties):\n",
    "            # Update the best emojis\n",
    "            best_emojis = emojis\n",
    "            best_n_grams = sent_combo\n",
    "            best_uncertainties = uncertainties[:]\n",
    "            \n",
    "    # Clear the function cache on closest_emoji because it is unlikely the next run will make use of them\n",
    "    closest_emoji.cache_clear()\n",
    "    \n",
    "    # Return the emoji \"sentence\", list of all the cosine similarities, and all of the n-grams\n",
    "    return (best_emojis, best_uncertainties, best_n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(sents, p_lemma_func, p_keep_stop_words):\n",
    "    global lemma_func\n",
    "    global keep_stop_words\n",
    "    lemma_func = p_lemma_func\n",
    "    keep_stop_words = p_keep_stop_words\n",
    "    generate_emoji_embeddings()\n",
    "    for sent in sents:\n",
    "        summarization_res = summarize(sent)\n",
    "        print(sent, \"|\", round(1 - sum(summarization_res[1])/len(summarization_res[1]), 3), \"|\", [round(x, 3) for x in summarization_res[1]] ,\"|\", summarization_res[2], \"|\", summarization_res[0] + \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christmas music rings from the clock tower | 0.983 | [0.0, 0.0, 0.0, 0.066] | ['christmas', 'music', 'ring', 'from the clock tower'] | 🎄🎻💍🏫|\n",
      "It not perfect but it is a start | 0.879 | [0.162, 0.323, 0.0, 0.0] | ['it not', 'perfect but it is', 'a', 'start'] | 🙅💯💯🌱|\n",
      "The sun is rising over new york city | 0.881 | [0.356, 0.0, 0.0] | ['the sun is rising over', 'new york', 'city'] | 🌄🗽🚏|\n"
     ]
    }
   ],
   "source": [
    "sents = [\"christmas music rings from the clock tower\", \"It not perfect but it is a start\", \"The sun is rising over new york city\"]\n",
    "format_summary(sents, lemmatizerNLTK.lemmatize, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
