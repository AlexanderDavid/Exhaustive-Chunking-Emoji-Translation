{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Plan\n",
    "The general metric we are using in this test plan is that the computer should translate a sentence into a series of emojis that a human can translate back into a single sentence. To do this we will split this into a couple of parts:\n",
    "   \n",
    "   1. Generate 100 sentences\n",
    "   2. Summarize each of the sentences\n",
    "   3. Take the top 20 sentences, sorted by the certainty score\n",
    "   4. For each machine translated sentence:\n",
    "       1. Provide the user with the emojis\n",
    "       2. Provide the user with an approximate sentence length\n",
    "       3. Prompt the user to tranlate the emojis into a sentence\n",
    "   5. For each machine translated sentence-user translated sentence pair:\n",
    "       1. Calculate the distance between the two sentences using sent2vec (might need another metric)\n",
    "\n",
    "We then will have a list of scores for each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation\n",
    "The sentences are gathered from the Stanford NLP research group's NMT dataset. The ones that we are currently using are located at https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2012.en. All of these sentences will be loaded into memory and cleaned if neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 sentences in dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the sentences\n",
    "file_path = \"data/tst2012.en\"\n",
    "testing_sentences = []\n",
    "with open(file_path, \"r\") as sents:\n",
    "    testing_sentences = [sent for sent in sents]\n",
    "            \n",
    "# Filter the sentences based on less than ten words\n",
    "word_limit_lower = 5\n",
    "word_limit_upper = 5\n",
    "testing_sentences = list(filter(lambda sent: len(word_tokenize(sent)) <= word_limit_upper and \n",
    "                                             len(word_tokenize(sent)) >= word_limit_lower, testing_sentences))\n",
    "\n",
    "# Some of the sentences have &apos; instead of ' but our algorithm doesn't handle that so replace with\n",
    "# regular \"'\"\n",
    "testing_sentences = [testing_sentence.replace(\"&apos;\", \"'\") for testing_sentence in testing_sentences]\n",
    "\n",
    "# We want 100 sentences at least\n",
    "print(f\"{len(testing_sentences)} sentences in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Summarization\n",
    "To do this we will just be using an exported Python V1 program that is just the NaiveEmojiTranslation notebook exported to .py. We summarize with the current best known params based on some limited observation. The sentence will be summarized using the best currently known parameters, and then the summaries scored based on the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance gives warnings when div by 0\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "# Exported NaiveEmojiTranslation to Python file as of October 24th\n",
    "from NaiveEmojiTranslation_V1 import summarize, lemmatizerNLTK\n",
    "# Sort the sentences by their uncertainty scores. This is imported as a generic scoring\n",
    "# function so that it can be swapped in and out easily\n",
    "from NaiveEmojiTranslation_V1 import score_summarization_result_average as scoring_function\n",
    "\n",
    "# JUST FOR TESTING ONLY USE TEN\n",
    "testing_sentences = testing_sentences[:10]\n",
    "\n",
    "# Summarize each testing sentence with the current best known parameters\n",
    "summarized_sentences = []\n",
    "for sentence in testing_sentences:\n",
    "    summarized_sentences.append(summarize(sentence, keep_stop_words=True, \n",
    "                                  lemma_func=lemmatizerNLTK.lemmatize, scoring_func=scoring_function))\n",
    "    \n",
    "# Sort the list by the scoring function\n",
    "summarized_sentences_sorted = list(sorted(summarized_sentences, key=scoring_function))\n",
    "\n",
    "# Choose only the top 30 summaries\n",
    "testing_summaries = summarized_sentences_sorted[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: üôè\n",
      "Sentence Length: 4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserSummarization(machine_summarization=EmojiSummarizationResult(emojis='üôè', n_grams=['thank you very much'], uncertainty_scores=[0.15913492441177368], elapsed_time=0), user_guess='thank you', difference=-1)\n"
     ]
    }
   ],
   "source": [
    "from NaiveEmojiTranslation_V1 import EmojiSummarizationResult\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class UserSummarization:\n",
    "    machine_summarization: EmojiSummarizationResult\n",
    "    user_guess: str = \"\"\n",
    "    difference: float = -1\n",
    "\n",
    "user_summaries = []\n",
    "for summary in summarized_sentences_sorted:\n",
    "    print(f\"Emoji Sequence: {summary.emojis}\")\n",
    "    print(\"Sentence Length: {}\".format(len(word_tokenize(\" \".join(summary.n_grams)))))\n",
    "    \n",
    "    translation = input(\"What's your translation?\")\n",
    "    \n",
    "    user_summaries.append(UserSummarization(summary, translation))\n",
    "    print(user_summaries[-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sent2vec model\n",
    "import sent2vec\n",
    "s2v = sent2vec.Sent2vecModel()\n",
    "s2v.load_model('../models/wiki_unigrams.bin') # https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User guessed: thank you\n",
      "Summary Input: thank you very much\n",
      "Difference: 0.15913492441177368\n",
      "Average cosine difference  0.15913492441177368\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine # Distance between sentence and emoji in sent2vec vector space\n",
    "\n",
    "for user_summary in user_summaries:\n",
    "    user_emb, mach_emb = s2v.embed_sentences([user_summary.user_guess, \" \".join(user_summary.machine_summarization.n_grams)])\n",
    "    user_summary.difference = cosine(user_emb, mach_emb)\n",
    "    print(\"User guessed: {}\\nSummary Input: {}\\nDifference: {}\".format(user_summary.user_guess, \" \".join(user_summary.machine_summarization.n_grams), user_summary.difference))\n",
    "    \n",
    "print(\"Average cosine difference \", sum([user_summary.difference for user_summary in user_summaries]) / len(user_summaries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
