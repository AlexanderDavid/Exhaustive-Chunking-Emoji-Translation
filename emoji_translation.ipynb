{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List, Tuple, Callable\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import spacy\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiTranslation(ABC):\n",
    "    @abstractmethod\n",
    "    def summarize(sent: str) -> str:\n",
    "        \"\"\"\n",
    "        Summarize the given sentence into emoji\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class NGramGroupingTranslation(EmojiTranslation):\n",
    "    sent2vec_model = None\n",
    "    def __init__(self, emoji_file: str=\"emoji_joined.txt\", sent2vec_model: str=\"../models/wiki_unigrams.bin\", lemmatizer: Callable[[str],str]=WordNetLemmatizer().lemmatize, remove_stops: bool=False):\n",
    "        self.emoji_file = emoji_file\n",
    "        \n",
    "        if NGramGroupingTranslation.sent2vec_model is None:\n",
    "            self.s2v = sent2vec.Sent2vecModel()\n",
    "            self.s2v.load_model(sent2vec_model)\n",
    "            NGramGroupingTranslation.sent2vec_model = self.s2v\n",
    "        else:\n",
    "            self.s2v = NGramGroupingTranslation.sent2vec_model\n",
    "        \n",
    "        self.lemma_func = lemmatizer\n",
    "        self.remove_stops = remove_stops\n",
    "        self.emoji_embeddings = self.load_emoji_embeddings()\n",
    "    \n",
    "    def clean_sentence(self, sent: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the given sentence using the given lemmatization technique and \n",
    "        removing stop words if the flag is set\n",
    "        \n",
    "        Args:\n",
    "            sent(str): Sentence to clean\n",
    "        Rets:\n",
    "            (str): Cleaned sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        return \" \".join([self.lemma_func(token) for token in word_tokenize(sent) if not self.remove_stops or token not in stopwords])\n",
    "    \n",
    "    def load_emoji_embeddings(self) -> List[Tuple[str, List[float]]]:\n",
    "        \"\"\"\n",
    "        Load the emoji embeddings by embedding the emoji definitions loaded in from a file\n",
    "        \"\"\"\n",
    "        \n",
    "        emoji_embeddings = []\n",
    "        with open(self.emoji_file) as emojis:\n",
    "            for defn in emojis:\n",
    "                split = defn.split(\"\\t\")\n",
    "                \n",
    "                # Get the emoji and the description from the current line\n",
    "                emoji = split[-1].replace(\"\\n\", \"\")\n",
    "                desc = self.clean_sentence(split[0])\n",
    "\n",
    "                # Add each emoji and embedded description to the list\n",
    "                emoji_embeddings.append((emoji, self.s2v.embed_sentence(desc)))\n",
    "        \n",
    "        return emoji_embeddings\n",
    "    \n",
    "    @lru_cache(maxsize=100)\n",
    "    def closest_emoji(self, sent: str) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Get the closest emoji to the given sentence\n",
    "\n",
    "        Args:\n",
    "            sent(List[str]): Sentence to check\n",
    "        Ret:\n",
    "            (Tuple[str, int]) Closest emoji, the respective cosine similarity\n",
    "\n",
    "        \"\"\"    \n",
    "        # Embed the sentence using sent2vec \n",
    "        emb = self.s2v.embed_sentence(sent)\n",
    "\n",
    "        # Start the lowest cosine at higher than it could ever be\n",
    "        lowest_cos = 1_000_000\n",
    "\n",
    "        # The best emoji starts as an empty string placeholder\n",
    "        best_emoji = \"\"\n",
    "\n",
    "        # Loop through the dictionary\n",
    "        for emoji in self.emoji_embeddings:\n",
    "            # Get the current emoji's embedding\n",
    "            emoji_emb = emoji[1]\n",
    "\n",
    "            # Check the cosine difference between the emoji's embedding and\n",
    "            # the sentence's embedding\n",
    "            curr_cos = cosine(emoji_emb, emb)\n",
    "\n",
    "            # If it lower than the lowest then it is the new best\n",
    "            if curr_cos < lowest_cos:\n",
    "                lowest_cos = curr_cos\n",
    "                best_emoji = emoji[0]\n",
    "\n",
    "        # Return a 2-tuple containing the best emoji and its cosine differnece\n",
    "        return best_emoji, lowest_cos\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_combinations(sent):\n",
    "        def combinations_of_sum(sum_to, combo=None):\n",
    "            combos = []\n",
    "            if combo is None:\n",
    "                combo = [1 for x in range(sum_to)]\n",
    "                combos.append(combo)\n",
    "\n",
    "            if len(combo) == 0:\n",
    "                return None\n",
    "\n",
    "            for i in range(1, len(combo)):\n",
    "                combo_to_query = combo[:i-1] + [sum(combo[i - 1:i + 1])] + combo[i+1:]\n",
    "                combos.append(combo_to_query)\n",
    "                [combos.append(combo) for combo in combinations_of_sum(sum_to, combo_to_query) if combo is not None]\n",
    "\n",
    "            return combos\n",
    "    \n",
    "        sent_combos = []\n",
    "        def combinations_of_sent_helper(sent):\n",
    "            sent = word_tokenize(sent)\n",
    "            combos = np.unique(combinations_of_sum(len(sent)))\n",
    "            sent_combos = []\n",
    "            for combo in combos:\n",
    "                sent_combo = []\n",
    "                curr_i = 0\n",
    "                for combo_len in combo:\n",
    "                    space_joined = \" \".join(sent[curr_i:combo_len + curr_i])\n",
    "                    if space_joined not in sent_combo:\n",
    "                        sent_combo.append(space_joined) \n",
    "                    curr_i += combo_len\n",
    "\n",
    "                if sent_combo not in sent_combos:\n",
    "                    sent_combos.append(sent_combo)\n",
    "            return sent_combos\n",
    "\n",
    "        return combinations_of_sent_helper(sent)\n",
    "\n",
    "    def summarize(self, sent:str) -> Tuple[List[str], List[float], List[str]]: \n",
    "        \"\"\"\n",
    "        Summarize the given sentence into emojis\n",
    "\n",
    "        Args:\n",
    "            sent(str): Sentence to summarize\n",
    "        Rets:\n",
    "            (Tuple[List[str], List[float], List[str]]): (Emoji Sentence, \n",
    "            List of Uncertainty values for the corresponding emoji,\n",
    "            list of n-grams used to generate the corresponding emoji)\n",
    "        \"\"\"\n",
    "        # Clean the sentence\n",
    "        sent = self.clean_sentence(sent)\n",
    "\n",
    "        # Generate all combinations of sentences\n",
    "        sent_combos = self.sentence_combinations(sent)\n",
    "        # Init \"best\" datamembers as empty or exceedingly high\n",
    "        best_emojis = \"\"\n",
    "        best_n_grams = []\n",
    "        best_uncertainties = [100_000_000]\n",
    "        # Iterate through every combination of sentence combos\n",
    "        for sent_combo in sent_combos:\n",
    "            # Start the local data members as empty\n",
    "            emojis = \"\"\n",
    "            uncertainties = []\n",
    "            # Iterate through each n_gram adding the uncertainty and emoji to the lists\n",
    "            for n_gram in sent_combo:\n",
    "                close_emoji, cos_diff = self.closest_emoji(n_gram)\n",
    "                emojis += close_emoji\n",
    "                uncertainties.append(cos_diff)\n",
    "\n",
    "            # Check if the average uncertainty is less than the best\n",
    "            # TODO: Maybe a median check would be helpful as well?\n",
    "            if sum(uncertainties)/len(uncertainties) < sum(best_uncertainties)/len(best_uncertainties):\n",
    "                # Update the best emojis\n",
    "                best_emojis = emojis\n",
    "                best_n_grams = sent_combo\n",
    "                best_uncertainties = uncertainties[:]\n",
    "\n",
    "        # Clear the function cache on closest_emoji because it is unlikely the next run will make use of them\n",
    "        closest_emoji.cache_clear()\n",
    "    \n",
    "        # Return the emoji \"sentence\", list of all the cosine similarities, and all of the n-grams\n",
    "        return (best_emojis, best_uncertainties, best_n_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nggt = NGramGroupingTranslation()\n",
    "nggt.summarize(\"christmas music rings from the clock tower\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
